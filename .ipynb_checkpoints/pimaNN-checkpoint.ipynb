{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network implementation\n",
    "1 hidden layer, 25 nodes in second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature normalization(FOR DATAFRAME, NOT NUMPY ARRAY)\n",
    "def featureNormalize(X, mu = 0, sigma = 0):\n",
    "    if((type(mu)==int or type(mu)== int) and (mu==0 or sigma==0)):\n",
    "        mu = np.mean(X)\n",
    "        sigma = np.std(X)\n",
    "        XNorm = (X - mu)/sigma\n",
    "        return(XNorm, mu, sigma)\n",
    "    else:\n",
    "        XNorm = (X - mu)/sigma\n",
    "        return(XNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no point in expanding features for NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1/(1+np.e**(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    #takes the Z values as input\n",
    "    #returns the sigmoid gradients.\n",
    "    gz = sigmoid(z)\n",
    "    sigG = gz * (1 - gz)\n",
    "    return(sigG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomly initialize weights\n",
    "One effective strategy for random initializa- tion is to randomly select values for Θ(l) uniformly in the range [−εinit, εinit]. This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt6}{\\sqrt{L_{in} + L_{out}}}$, where $L{in} = s_l$ and $L{out} = s_{l+1}$ are the number of units in the layer adjacent to $\\Theta^{(l)}$  \n",
    "$L{in} = s_l$ = number of nodes in current layer  \n",
    "$L{out} = s_{l+1}$ = number of nodes in the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(LIn, LOut):\n",
    "    #takes number of nodes in current layer LIn and next layer LOut\n",
    "    #EXCLUDING BIAS NODE\n",
    "    #returns the theta matrix in shape (LOut x (LIn + 1))\n",
    "    # +1 is the bias theta\n",
    "    epsilon = np.round(np.sqrt(6)/np.sqrt(LIn + LOut), 2)\n",
    "    \n",
    "    W = np.random.rand(LOut, LIn + 1) * 2 * epsilon - epsilon\n",
    "    return(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, no_labels, X, y, lam):\n",
    "    #X passed to the function should be a UNBIASED DF\n",
    "    # y should be a DF\n",
    "    #nn_params includes the bias unit theta.\n",
    "    \n",
    "    #+ 1 to include the theta for bias unit\n",
    "    Theta1 = nn_params[:hidden_layer_size*(input_layer_size + 1)].reshape(hidden_layer_size, (input_layer_size + 1))\n",
    "    Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1) :].reshape(no_labels, (hidden_layer_size + 1))\n",
    "    \n",
    "    # X adding bias\n",
    "    m = X.shape[0]\n",
    "    if type(X) == np.ndarray:\n",
    "        ones = np.ones((m, 1))\n",
    "        X = np.concatenate((ones, X), axis = 1)\n",
    "    else:\n",
    "        x0 = pd.DataFrame(np.ones((m, 1)), columns=[\"x0\"], index=X.index)\n",
    "        X = pd.concat((x0, X), axis = 1)\n",
    "    \n",
    "    #If its binary classification, y does not need to be changed to a matrix\n",
    "    if no_labels == 1:\n",
    "        ymat = np.array(y, dtype=float)\n",
    "    else:\n",
    "        ymat = np.zeros((m, no_labels));\n",
    "        for i in range(m):\n",
    "            ymat[i, y[i]] = 1;\n",
    "\n",
    "    #hypothesis\n",
    "    hypo = np.zeros_like(ymat)\n",
    "    # cummulative theta derivatives initialization\n",
    "    accuDELTA1 = np.zeros_like(Theta1)\n",
    "    accuDELTA2 = np.zeros_like(Theta2)\n",
    "    \n",
    "    #For loop\n",
    "    for i in range(m):\n",
    "        # FORWARD PROPAGATION\n",
    "        #if X is a DF, necessary to change a1 to ndarray\n",
    "        if type(X) == np.ndarray:\n",
    "            a1 = X[i]\n",
    "        else:\n",
    "            a1 = np.array(X.iloc[i, :])\n",
    "        z2 = np.dot(a1, Theta1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        #add bias\n",
    "        a2 = np.append([1], a2)\n",
    "        z3 = np.dot(a2, Theta2.T)\n",
    "        a3 = sigmoid(z3);\n",
    "        hypo[i] = a3\n",
    "\n",
    "        #BACKWARD PROPAGATION\n",
    "        \n",
    "        #delta calculation, here the deltas are gradients or \n",
    "        # PARTIAL DERIVATIVES FOR ACTIVATION NODE a\n",
    "        delta3 = a3 - ymat[i];\n",
    "        delta2 = np.dot(delta3, Theta2) * (a2 * (1 - a2))\n",
    "        delta2 = delta2[1:]\n",
    "        \n",
    "        #sum of partial derivatives for theta parameters in all samples\n",
    "        accuDELTA2 += delta3.reshape(delta3.shape[0], 1) * a2\n",
    "        accuDELTA1 += delta2.reshape(delta2.shape[0], 1) * a1\n",
    "    \n",
    "    #devide the sum of theta partial derivatives by m to acquire\n",
    "    #theta gradients\n",
    "    Theta1_grad = accuDELTA1 / m\n",
    "    Theta2_grad = accuDELTA2 / m\n",
    "\n",
    "    #add regularization term gradient EXCLUDING THE BIAS theta\n",
    "    #because a0 is already very small(1),\n",
    "    # regularizing their theta will shrink a0 to 0\n",
    "    Theta1_grad[:, 1:] += lam/m * Theta1[:, 1:]\n",
    "    Theta2_grad[:, 1:] += lam/m * Theta2[:, 1:]\n",
    "    \n",
    "    #flatten out gradients\n",
    "    grads = np.append(Theta1_grad.flatten(), Theta2_grad.flatten())\n",
    "    \n",
    "    Junregularized = 1/m * ((-ymat) * np.log(hypo) - (1 - ymat) * np.log(1- hypo))\n",
    "    Junregularized = np.sum(Junregularized)\n",
    "    \n",
    "    #add regularization term\n",
    "    Jregularization = lam / (2 * m)*(np.sum(Theta1[:, 1:]**2) + np.sum(Theta2[:, 1:]**2))\n",
    "    J = Junregularized + Jregularization;\n",
    "\n",
    "    return(J, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(input_layer_size, hidden_layer_size, no_labels, X, y, initial_nn_params, lam, Method):\n",
    "\n",
    "    options={'disp': False, 'gtol': 1e-6} #'gtol':1e-6, 'maxIters': 50,     for BFGS\n",
    "    minimizeArgs = (input_layer_size, hidden_layer_size,no_labels, X, y, lam)\n",
    "\n",
    "    t1 = time.time()\n",
    "    res = minimize(nnCostFunction, initial_nn_params, method= Method, jac=True,  options=options, args=minimizeArgs) #  callback=printx,  # CG potentially fastest\n",
    "    t2 = time.time()\n",
    "    #print('\\ntotal time: ' + str(t2 - t1), end = '\\r')\n",
    "    itera = 1\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X, threshold = 0.5):\n",
    "    #m, width, height = X.shape\n",
    "    m, n = X.shape\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    #X = X.reshape(m, width*height)\n",
    "    \n",
    "    ones = np.ones((m, 1))\n",
    "    a1 = np.concatenate((ones, X), axis = 1)\n",
    "\n",
    "    h1 = sigmoid(np.dot(a1,optimizedTheta1.T))\n",
    "    h1 = np.concatenate((ones, h1), axis = 1)\n",
    "    \n",
    "    h2 = sigmoid(np.dot(h1,optimizedTheta2.T))\n",
    "    \n",
    "    if h2.shape[1] > 1: # K-classification\n",
    "        #argmax(finds the indeces of maximum values along axis 1, along 2nd dimension)\n",
    "        prediction = h2.argmax(axis=1).reshape((m, 1))\n",
    "    else: # binary classification\n",
    "        prediction = h2>threshold\n",
    "        # convert boolean to value\n",
    "        prediction = (prediction * 1).flatten()\n",
    "    return(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and track number of entries in sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"pima-indians-diabetes.data.txt\", names=['NoPregnant', 'Plasma glucose concentration', 'Diastolic blood pressure(mm Hg)', 'Triceps skin fold thickness (mm)', '2-hr serum insulin(mu U/ml)', 'Body mass index', 'Diabetes pedigree function', 'Age', 'Class variable'])\n",
    "\n",
    "#xTrain, xCV, xTest\n",
    "X = data[data.columns[:-1]]\n",
    "y = pd.DataFrame(data[data.columns[-1]])\n",
    "m = X.shape[0]\n",
    "\n",
    "mTemp = np.round(X.shape[0]*0.4)\n",
    "if mTemp%2 != 0:\n",
    "    mTemp+=1\n",
    "mCV = mTest = int(mTemp/2)\n",
    "mTrain = int(m - mTemp)\n",
    "\n",
    "#splitting dataset\n",
    "XTrain = X.iloc[:mTrain]\n",
    "yTrain = y.iloc[:mTrain]\n",
    "XCV = X.iloc[mTrain:mTrain+mCV]\n",
    "yCV = y.iloc[mTrain:mTrain+mCV]\n",
    "XTest = X.iloc[-mCV:]\n",
    "yTest = y.iloc[-mCV:]\n",
    "\n",
    "\n",
    "#X normalization and bias addition\n",
    "# NO NEED TO ADD BIAS TERM, because the randInitTheta will need to add extra bias theta,\n",
    "#so if the input layer is biased, while the rest is not, it will\n",
    "# make the algorithm more complicated and have redundent cases\n",
    "XTrain, mu, sigma = featureNormalize(XTrain)\n",
    "\n",
    "#XCV configuration\n",
    "XCV = featureNormalize(XCV, mu, sigma)\n",
    "\n",
    "#XTest configuration\n",
    "XTest = featureNormalize(XTest, mu, sigma)\n",
    "\n",
    "#nTrain = n + bias unit\n",
    "nTrain = XTrain.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters and splitting sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN params\n",
    "input_layer_size = nTrain\n",
    "hidden_layer_size = 25\n",
    "no_labels = 1\n",
    "lam = 1\n",
    "\n",
    "# Theta initialization, after initialization the thetas will include the bias unit\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, no_labels)\n",
    "\n",
    "initial_nn_params = np.append(initial_Theta1.flatten(), initial_Theta2.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77821800732208346"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost, grad = nnCostFunction(initial_nn_params, input_layer_size, hidden_layer_size,no_labels, XTrain, yTrain, lam)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = learn(input_layer_size, hidden_layer_size, no_labels, XTrain, yTrain,initial_nn_params, lam, 'CG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put optimized thetas back into a matrix form\n",
    "optimizedTheta1 = res.x[:hidden_layer_size*(input_layer_size + 1)].reshape(hidden_layer_size, (input_layer_size + 1))\n",
    "optimizedTheta2 = res.x[hidden_layer_size * (input_layer_size + 1 ) :].reshape(no_labels, (hidden_layer_size + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 0.786956521739\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(optimizedTheta1, optimizedTheta2, XTrain)\n",
    "print(\"prediction accuracy: \" + str(np.sum(prediction == yTrain['Class variable'])/mTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x111f652b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111eeca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsetSize = 100\n",
    "trainError = np.zeros((subsetSize, 1))\n",
    "CVError = np.zeros((subsetSize, 1))\n",
    "for i in range(1, subsetSize + 1):\n",
    "    res = learn(input_layer_size, hidden_layer_size, no_labels, XTrain[:i], yTrain[:i],initial_nn_params, lam, 'CG').x\n",
    "    trainError[i-1] = nnCostFunction(res, input_layer_size, hidden_layer_size,no_labels, XTrain[:i], yTrain[:i], 0)[0]\n",
    "    CVError[i-1] = nnCostFunction(res, input_layer_size, hidden_layer_size,no_labels, XCV, yCV, 0)[0]\n",
    "plt.plot(range(subsetSize), trainError, label = 'train')\n",
    "plt.plot(range(subsetSize), CVError, label = 'CV')\n",
    "plt.legend(loc = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different NN architecture\n",
    "### 1 hidden layer is a good default\n",
    "### multiple hidden layer\n",
    "- Then each layer should have same no. hidden units(usually the more the better)\n",
    "- Having more hidden units is computationally expensive\n",
    "- often having more hidden units is very good\n",
    "- __Usually no of hidden units is comparable to dimension of X or two or three times of that__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN params\n",
    "input_layer_size = nTrain\n",
    "no_hidden_layers = 2\n",
    "#usually hidden units is comparable to dimension of X or two or three times\n",
    "hidden_layer_size = nTrain\n",
    "no_labels = 1\n",
    "lam = 1\n",
    "\n",
    "# itera\n",
    "itera = 1\n",
    "\n",
    "#initial_thetas is the array of thetas for all the layers\n",
    "#initial_nn_params is the array of all thetas flattened\n",
    "initial_thetas = []\n",
    "for i in range(no_hidden_layers + 1):\n",
    "    if i == 0:# input layer matrix\n",
    "        initial_thetas.append(randInitializeWeights(input_layer_size, hidden_layer_size))\n",
    "        initial_nn_params = np.array(initial_thetas).flatten()\n",
    "    else:\n",
    "        if(i != no_hidden_layers): #hidden layer matrix\n",
    "            temp = randInitializeWeights(hidden_layer_size, hidden_layer_size)\n",
    "            initial_thetas.append(temp)\n",
    "        else:#matrix before output layer\n",
    "            temp = randInitializeWeights(hidden_layer_size, no_labels)\n",
    "            initial_thetas.append(temp)\n",
    "        initial_nn_params = np.append(initial_nn_params, temp.flatten())\n",
    "initial_thetas = np.array(initial_thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom NN architecture cost function\n",
    "## I dont bother with it anymore......start using libraries..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check backprop is properly implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkNNGradients\n",
    "def checkNNGradients(lam = 0):\n",
    "    input_layer_size = 3;\n",
    "    hidden_layer_size = 5;\n",
    "    num_labels = 3;\n",
    "    m = 5;\n",
    "    \n",
    "    #random test data generation\n",
    "    Theta1 = debugInitialWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitialWeights(num_labels, hidden_layer_size)\n",
    "    \n",
    "    X = debugInitialWeights(m, input_layer_size-1)\n",
    "    y = np.mod(np.arange(1, 6), num_labels) \n",
    "    \n",
    "    nn_params = np.append(Theta1.flatten(), Theta2.flatten())\n",
    "    \n",
    "    cost, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,num_labels, X, y, lam)\n",
    "    \n",
    "    #args = (nn_params,input_layer_size, hidden_layer_size,no_labels, X, y, lam)\n",
    "    \n",
    "    numgrad = computeNumericalGradient(nnCostFunction, nn_params,input_layer_size, hidden_layer_size,num_labels, X, y, lam)\n",
    "    for i in range(len(grad)):\n",
    "        print( grad[i], numgrad[i])\n",
    "    diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad)\n",
    "    print(\"If back prop is properly implemented, difference should be small, less than 1e-9.\")\n",
    "    \n",
    "    print(\"relative difference: \\n\" + str(diff))\n",
    "\n",
    "#debugInitialWeights\n",
    "def debugInitialWeights(fan_out, fan_in):\n",
    "    w = np.zeros((fan_out, 1 + fan_in))\n",
    "    w = np.sin(np.arange(1,np.size(w)+1)).reshape(fan_out, 1 + fan_in)\n",
    "    return(w)\n",
    "\n",
    "#computeNumericalGradient\n",
    "def computeNumericalGradient(costFun, theta,input_layer_size, hidden_layer_size,no_labels, X, y, lam):\n",
    "    numgrad = np.zeros_like(theta);\n",
    "    perturb = np.zeros_like(theta);\n",
    "    e = 10**-4\n",
    "    for p in range(theta.size):\n",
    "        perturb[p] = e\n",
    "        loss1 = costFun(theta - perturb, input_layer_size, hidden_layer_size,no_labels, X, y, lam)[0];\n",
    "        loss2 = costFun(theta + perturb, input_layer_size, hidden_layer_size,no_labels, X, y, lam)[0];\n",
    "        \n",
    "        numgrad[p] = (loss2 - loss1)/(2*e)\n",
    "        perturb[p] = 0\n",
    "    return(numgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.152395925485 0.152395925381\n",
      "0.00867455483106 0.0086745548411\n",
      "0.0167388729772 0.016738873001\n",
      "0.00941354850338 0.00941354850292\n",
      "0.0572621360189 0.057262135984\n",
      "0.0301513760063 0.0301513759915\n",
      "0.0401924661746 0.0401924661331\n",
      "0.013280788299 0.0132807882935\n",
      "-0.0742315718998 -0.0742315718938\n",
      "-0.0160152856103 -0.0160152855955\n",
      "-0.027816259377 -0.0278162593426\n",
      "-0.0140430925538 -0.014043092551\n",
      "-0.136235888501 -0.136235888484\n",
      "-0.00304407820088 -0.00304407823171\n",
      "-0.00685651520662 -0.00685651529553\n",
      "-0.00436510375183 -0.00436510376067\n",
      "-0.0726020435824 -0.0726020435415\n",
      "-0.0216340281404 -0.0216340281378\n",
      "-0.0294166882597 -0.0294166882453\n",
      "-0.010153780855 -0.0101537808517\n",
      "0.416549362562 0.416549362467\n",
      "0.280953791292 0.280953791256\n",
      "0.117434013596 0.117434013589\n",
      "0.252954161989 0.25295416197\n",
      "0.22418550443 0.224185504394\n",
      "0.116730344884 0.116730344881\n",
      "0.210129829549 0.210129829463\n",
      "0.135147414117 0.135147414084\n",
      "0.108455538277 0.10845553827\n",
      "0.0709028000321 0.0709028000134\n",
      "0.143024079327 0.143024079295\n",
      "0.0829384045449 0.082938404542\n",
      "0.195064368703 0.195064368629\n",
      "0.168360863558 0.168360863531\n",
      "0.0605892845858 0.0605892845806\n",
      "0.0717923004295 0.0717923004112\n",
      "0.157848726433 0.157848726403\n",
      "0.0308215356424 0.0308215356415\n",
      "If back prop is properly implemented, difference should be small, less than 1e-9.\n",
      "relative difference: \n",
      "1.46060952051e-10\n"
     ]
    }
   ],
   "source": [
    "checkNNGradients()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
