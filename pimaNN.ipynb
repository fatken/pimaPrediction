{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network implementation\n",
    "1 hidden layer, 25 nodes in second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature normalization(FOR DATAFRAME, NOT NUMPY ARRAY)\n",
    "def featureNormalize(X, mu = 0, sigma = 0):\n",
    "    if((type(mu)==int or type(mu)== int) and (mu==0 or sigma==0)):\n",
    "        mu = np.mean(X)\n",
    "        sigma = np.std(X)\n",
    "        XNorm = (X - mu)/sigma\n",
    "        return(XNorm, mu, sigma)\n",
    "    else:\n",
    "        XNorm = (X - mu)/sigma\n",
    "        return(XNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no point in expanding features for NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1/(1+np.e**(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    #takes the Z values as input\n",
    "    #returns the sigmoid gradients.\n",
    "    gz = sigmoid(z)\n",
    "    sigG = gz * (1 - gz)\n",
    "    return(sigG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomly initialize weights\n",
    "One effective strategy for random initializa- tion is to randomly select values for Θ(l) uniformly in the range [−εinit, εinit]. This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt6}{\\sqrt{L_{in} + L_{out}}}$, where $L{in} = s_l$ and $L{out} = s_{l+1}$ are the number of units in the layer adjacent to $\\Theta^{(l)}$  \n",
    "$L{in} = s_l$ = number of nodes in current layer  \n",
    "$L{out} = s_{l+1}$ = number of nodes in the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(LIn, LOut):\n",
    "    #takes number of nodes in current layer LIn and next layer LOut\n",
    "    #EXCLUDING BIAS NODE\n",
    "    #returns the theta matrix in shape (LOut x (LIn + 1))\n",
    "    # +1 is the bias theta\n",
    "    epsilon = np.round(np.sqrt(6)/np.sqrt(LIn + LOut), 2)\n",
    "    \n",
    "    W = np.random.rand(LOut, LIn + 1) * 2 * epsilon - epsilon\n",
    "    return(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, no_labels, X, y, lam):\n",
    "    #X passed to the function should be a UNBIASED DF\n",
    "    # y should be a DF\n",
    "    #nn_params includes the bias unit theta.\n",
    "    #t1 = time.time()\n",
    "    \n",
    "    #+ 1 because thetas include the bias unit\n",
    "    Theta1 = nn_params[:hidden_layer_size*(input_layer_size + 1)].reshape(hidden_layer_size, (input_layer_size + 1))\n",
    "    Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1) :].reshape(no_labels, (hidden_layer_size + 1))\n",
    "    \n",
    "    # X adding bias\n",
    "    m = X.shape[0]\n",
    "    if type(X) == np.ndarray:\n",
    "        ones = np.ones((m, 1))\n",
    "        X = np.concatenate((ones, X), axis = 1)\n",
    "    else:\n",
    "        x0 = pd.DataFrame(np.ones((m, 1)), columns=[\"x0\"], index=X.index)\n",
    "        X = pd.concat((x0, X), axis = 1)\n",
    "    \n",
    "    #If its binary classification, y does not need to be changed to a matrix\n",
    "    if no_labels == 1:\n",
    "        ymat = np.array(y, dtype=float)\n",
    "    else:\n",
    "        ymat = np.zeros((m, no_labels));\n",
    "        for i in range(m):\n",
    "            ymat[i, y[i]] = 1;\n",
    "\n",
    "    #hypothesis\n",
    "    hypo = np.zeros_like(ymat)\n",
    "    # cummulative theta derivatives initialization\n",
    "    accuDELTA1 = np.zeros_like(Theta1)\n",
    "    accuDELTA2 = np.zeros_like(Theta2)\n",
    "    \n",
    "    #For loop\n",
    "    for i in range(m):\n",
    "        # FORWARD PROPAGATION\n",
    "        #if X is a DF, necessary to change a1 to ndarray\n",
    "        if type(X) == np.ndarray:\n",
    "            a1 = X[i]\n",
    "        else:\n",
    "            a1 = np.array(X.iloc[i, :])\n",
    "        z2 = np.dot(a1, Theta1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        #add bias\n",
    "        a2 = np.append([1], a2)\n",
    "        z3 = np.dot(a2, Theta2.T)\n",
    "        a3 = sigmoid(z3);\n",
    "        hypo[i] = a3\n",
    "\n",
    "        #BACKWARD PROPAGATION\n",
    "        \n",
    "        #delta calculation, here the deltas are gradients or \n",
    "        # PARTIAL DERIVATIVES FOR ACTIVATION NODE a\n",
    "        delta3 = a3 - ymat[i];\n",
    "        delta2 = np.dot(delta3, Theta2) * (a2 * (1 - a2))\n",
    "        delta2 = delta2[1:]\n",
    "        \n",
    "        #sum of partial derivatives for theta parameters in all samples\n",
    "        accuDELTA2 += delta3.reshape(delta3.shape[0], 1) * a2\n",
    "        accuDELTA1 += delta2.reshape(delta2.shape[0], 1) * a1\n",
    "    \n",
    "    #devide the sum of theta partial derivatives by m to acquire\n",
    "    #theta gradients\n",
    "    Theta1_grad = accuDELTA1 / m\n",
    "    Theta2_grad = accuDELTA2 / m\n",
    "\n",
    "    #add regularization term gradient EXCLUDING THE BIAS theta\n",
    "    #because a0 is already very small(1),\n",
    "    # regularizing their theta will shrink a0 to 0\n",
    "    Theta1_grad[:, 1:] += lam/m * Theta1[:, 1:]\n",
    "    Theta2_grad[:, 1:] += lam/m * Theta2[:, 1:]\n",
    "    \n",
    "    #flatten out gradients\n",
    "    grads = np.append(Theta1_grad.flatten(), Theta2_grad.flatten())\n",
    "    \n",
    "    Junregularized = 1/m * ((-ymat) * np.log(hypo) - (1 - ymat) * np.log(1- hypo))\n",
    "    Junregularized = np.sum(Junregularized)\n",
    "    \n",
    "    #add regularization term\n",
    "    Jregularization = lam / (2 * m)*(np.sum(Theta1[:, 1:]**2) + np.sum(Theta2[:, 1:]**2))\n",
    "    J = Junregularized + Jregularization;\n",
    "\n",
    "    return(J, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(input_layer_size, hidden_layer_size, no_labels, X, y, initial_nn_params, lam, Method):\n",
    "\n",
    "    global itera\n",
    "    options={'disp': False, 'gtol': 1e-6} #'gtol':1e-6, 'maxIters': 50,     for BFGS\n",
    "    minimizeArgs = (input_layer_size, hidden_layer_size,no_labels, X, y, lam)\n",
    "\n",
    "    t1 = time.time()\n",
    "    res = minimize(nnCostFunction, initial_nn_params, method= Method, jac=True,  options=options, args=minimizeArgs) #  callback=printx,  # CG potentially fastest\n",
    "    t2 = time.time()\n",
    "    print('\\ntotal time: ' + str(t2 - t1), end = '\\r')\n",
    "    itera = 1\n",
    "    return(res)\n",
    "\n",
    "#callback function\n",
    "    def printx(Xi):\n",
    "        global itera\n",
    "        print(itera, end = '\\r',)\n",
    "        itera += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X, threshold = 0.5):\n",
    "    #m, width, height = X.shape\n",
    "    m, n = X.shape\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    #X = X.reshape(m, width*height)\n",
    "    \n",
    "    ones = np.ones((m, 1))\n",
    "    a1 = np.concatenate((ones, X), axis = 1)\n",
    "\n",
    "    h1 = sigmoid(np.dot(a1,optimizedTheta1.T))\n",
    "    h1 = np.concatenate((ones, h1), axis = 1)\n",
    "    \n",
    "    h2 = sigmoid(np.dot(h1,optimizedTheta2.T))\n",
    "    \n",
    "    if h2.shape[1] > 1: # K-classification\n",
    "        #argmax(finds the indeces of maximum values along axis 1, along 2nd dimension)\n",
    "        prediction = h2.argmax(axis=1).reshape((m, 1))\n",
    "    else: # binary classification\n",
    "        prediction = h2>threshold\n",
    "        # convert boolean to value\n",
    "        prediction = (prediction * 1).flatten()\n",
    "    return(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and track number of entries in sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"pima-indians-diabetes.data.txt\", names=['NoPregnant', 'Plasma glucose concentration', 'Diastolic blood pressure(mm Hg)', 'Triceps skin fold thickness (mm)', '2-hr serum insulin(mu U/ml)', 'Body mass index', 'Diabetes pedigree function', 'Age', 'Class variable'])\n",
    "\n",
    "#xTrain, xCV, xTest\n",
    "X = data[data.columns[:-1]]\n",
    "y = pd.DataFrame(data[data.columns[-1]])\n",
    "m = X.shape[0]\n",
    "\n",
    "mTemp = np.round(X.shape[0]*0.4)\n",
    "if mTemp%2 != 0:\n",
    "    mTemp+=1\n",
    "mCV = mTest = int(mTemp/2)\n",
    "mTrain = int(m - mTemp)\n",
    "\n",
    "#splitting dataset\n",
    "XTrain = X.iloc[:mTrain]\n",
    "yTrain = y.iloc[:mTrain]\n",
    "XCV = X.iloc[mTrain:mTrain+mCV]\n",
    "yCV = y.iloc[mTrain:mTrain+mCV]\n",
    "XTest = X.iloc[-mCV:]\n",
    "yTest = y.iloc[-mCV:]\n",
    "\n",
    "\n",
    "#X normalization and bias addition\n",
    "# NO NEED TO ADD BIAS TERM, because the randInitTheta will need to add extra bias theta,\n",
    "#so if the input layer is biased, while the rest is not, it will\n",
    "# make the algorithm more complicated and have redundent cases\n",
    "XTrain, mu, sigma = featureNormalize(XTrain)\n",
    "\n",
    "#XCV configuration\n",
    "XCV = featureNormalize(XCV, mu, sigma)\n",
    "\n",
    "#XTest configuration\n",
    "XTest = featureNormalize(XTest, mu, sigma)\n",
    "\n",
    "#nTrain = n + bias unit\n",
    "nTrain = XTrain.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters and splitting sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN params\n",
    "input_layer_size = nTrain\n",
    "hidden_layer_size = 25\n",
    "no_labels = 1\n",
    "lam = 1\n",
    "\n",
    "# itera\n",
    "itera = 1\n",
    "\n",
    "# Theta initialization, after initialization the thetas will include the bias unit\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, no_labels)\n",
    "\n",
    "initial_nn_params = np.append(initial_Theta1.flatten(), initial_Theta2.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69995637625134244"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost, grad = nnCostFunction(initial_nn_params, input_layer_size, hidden_layer_size,no_labels, XTrain, yTrain, lam)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn(input_layer_size, hidden_layer_size, no_labels, XTrain, yTrain,initial_nn_params, lam, 'CG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put optimized thetas back into a matrix form\n",
    "optimizedTheta1 = res.x[:hidden_layer_size*(input_layer_size + 1)].reshape(hidden_layer_size, (input_layer_size + 1))\n",
    "optimizedTheta2 = res.x[hidden_layer_size * (input_layer_size + 1 ) :].reshape(no_labels, (hidden_layer_size + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 0.786956521739\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(optimizedTheta1, optimizedTheta2, XTrain)\n",
    "print(\"prediction accuracy: \" + str(np.sum(prediction == yTrain['Class variable'])/mTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x112df4fd0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11335fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsetSize = 100\n",
    "trainError = np.zeros((subsetSize, 1))\n",
    "CVError = np.zeros((subsetSize, 1))\n",
    "for i in range(1, subsetSize + 1):\n",
    "    res = learn(input_layer_size, hidden_layer_size, no_labels, XTrain[:i], yTrain[:i],initial_nn_params, lam, 'CG').x\n",
    "    trainError[i-1] = nnCostFunction(res, input_layer_size, hidden_layer_size,no_labels, XTrain[:i], yTrain[:i], 0)[0]\n",
    "    CVError[i-1] = nnCostFunction(res, input_layer_size, hidden_layer_size,no_labels, XCV, yCV, 0)[0]\n",
    "plt.plot(range(subsetSize), trainError, label = 'train')\n",
    "plt.plot(range(subsetSize), CVError, label = 'CV')\n",
    "plt.legend(loc = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different NN architecture\n",
    "### 1 hidden layer is a good default\n",
    "### multiple hidden layer\n",
    "- Then each layer should have same no. hidden units(usually the more the better)\n",
    "- Having more hidden units is computationally expensive\n",
    "- often having more hidden units is very good\n",
    "- __Usually no of hidden units is comparable to dimension of X or two or three times of that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check backprop is properly implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkNNGradients\n",
    "def checkNNGradients(lam = 0):\n",
    "    input_layer_size = 3;\n",
    "    hidden_layer_size = 5;\n",
    "    num_labels = 3;\n",
    "    m = 5;\n",
    "    \n",
    "    #random test data generation\n",
    "    Theta1 = debugInitialWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitialWeights(num_labels, hidden_layer_size)\n",
    "    \n",
    "    X = debugInitialWeights(m, input_layer_size-1)\n",
    "    y = np.mod(np.arange(1, 6), num_labels) \n",
    "    \n",
    "    nn_params = np.append(Theta1.flatten(), Theta2.flatten())\n",
    "    \n",
    "    cost, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,num_labels, X, y, lam)\n",
    "    \n",
    "    #args = (nn_params,input_layer_size, hidden_layer_size,no_labels, X, y, lam)\n",
    "    \n",
    "    numgrad = computeNumericalGradient(nnCostFunction, nn_params,input_layer_size, hidden_layer_size,num_labels, X, y, lam)\n",
    "    for i in range(len(grad)):\n",
    "        print( grad[i], numgrad[i])\n",
    "    diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad)\n",
    "    print(\"If back prop is properly implemented, difference should be small, less than 1e-9.\")\n",
    "    \n",
    "    print(\"relative difference: \\n\" + str(diff))\n",
    "\n",
    "#debugInitialWeights\n",
    "def debugInitialWeights(fan_out, fan_in):\n",
    "    w = np.zeros((fan_out, 1 + fan_in))\n",
    "    w = np.sin(np.arange(1,np.size(w)+1)).reshape(fan_out, 1 + fan_in)\n",
    "    return(w)\n",
    "\n",
    "#computeNumericalGradient\n",
    "def computeNumericalGradient(costFun, theta,input_layer_size, hidden_layer_size,no_labels, X, y, lam):\n",
    "    numgrad = np.zeros_like(theta);\n",
    "    perturb = np.zeros_like(theta);\n",
    "    e = 10**-4\n",
    "    for p in range(theta.size):\n",
    "        perturb[p] = e\n",
    "        loss1 = costFun(theta - perturb, input_layer_size, hidden_layer_size,no_labels, X, y, lam)[0];\n",
    "        loss2 = costFun(theta + perturb, input_layer_size, hidden_layer_size,no_labels, X, y, lam)[0];\n",
    "        \n",
    "        numgrad[p] = (loss2 - loss1)/(2*e)\n",
    "        perturb[p] = 0\n",
    "    return(numgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.152395925485 0.152395925381\n",
      "0.00867455483106 0.0086745548411\n",
      "0.0167388729772 0.016738873001\n",
      "0.00941354850338 0.00941354850292\n",
      "0.0572621360189 0.057262135984\n",
      "0.0301513760063 0.0301513759915\n",
      "0.0401924661746 0.0401924661331\n",
      "0.013280788299 0.0132807882935\n",
      "-0.0742315718998 -0.0742315718938\n",
      "-0.0160152856103 -0.0160152855955\n",
      "-0.027816259377 -0.0278162593426\n",
      "-0.0140430925538 -0.014043092551\n",
      "-0.136235888501 -0.136235888484\n",
      "-0.00304407820088 -0.00304407823171\n",
      "-0.00685651520662 -0.00685651529553\n",
      "-0.00436510375183 -0.00436510376067\n",
      "-0.0726020435824 -0.0726020435415\n",
      "-0.0216340281404 -0.0216340281378\n",
      "-0.0294166882597 -0.0294166882453\n",
      "-0.010153780855 -0.0101537808517\n",
      "0.416549362562 0.416549362467\n",
      "0.280953791292 0.280953791256\n",
      "0.117434013596 0.117434013589\n",
      "0.252954161989 0.25295416197\n",
      "0.22418550443 0.224185504394\n",
      "0.116730344884 0.116730344881\n",
      "0.210129829549 0.210129829463\n",
      "0.135147414117 0.135147414084\n",
      "0.108455538277 0.10845553827\n",
      "0.0709028000321 0.0709028000134\n",
      "0.143024079327 0.143024079295\n",
      "0.0829384045449 0.082938404542\n",
      "0.195064368703 0.195064368629\n",
      "0.168360863558 0.168360863531\n",
      "0.0605892845858 0.0605892845806\n",
      "0.0717923004295 0.0717923004112\n",
      "0.157848726433 0.157848726403\n",
      "0.0308215356424 0.0308215356415\n",
      "If back prop is properly implemented, difference should be small, less than 1e-9.\n",
      "relative difference: \n",
      "1.46060952051e-10\n"
     ]
    }
   ],
   "source": [
    "checkNNGradients()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
